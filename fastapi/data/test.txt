extracted_text": "Page 1:\narXiv:1910.14296v2 [cs.CL] 6 Oct 2020\nLIMIT-BERT: Linguistics Informed Multi-Task BERT\n1,2,3\nJunru Zhou1,2,3, Zhuosheng Zhang 1,2,3, Hai Zhao1,2,3*, Shuailiang Zhang\n1\n¹Department of Computer Science and Engineering, Shanghai Jiao Tong University\n2 Key Laboratory of Shanghai Education Commission for Intelligent Interaction\nand Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China\n3 MOE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University\n{zhoujunru, zhangzs}@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn\nAbstract\nIn this paper, we present Linguistics Informed\nMulti-Task BERT (LIMIT-BERT) for learning\nlanguage representations across multiple lin-\nguistics tasks by Multi-Task Learning. LIMIT-\nBERT includes five key linguistics tasks: Part-\nOf-Speech (POS) tags, constituent and de-\npendency syntactic parsing, span and depen-\ndency semantic role labeling (SRL). Differ-\nent from recent Multi-Task Deep Neural Net-\nworks (MT-DNN), our LIMIT-BERT is fully\nlinguistics motivated and thus is capable of\nadopting an improved masked training objec-\ntive according to syntactic and semantic con-\nstituents. Besides, LIMIT-BERT takes a semi-\nsupervised learning strategy to offer the same\nlarge amount of linguistics task data as that\nfor the language model training. As a re-\nsult, LIMIT-BERT not only improves linguis-\ntics tasks performance, but also benefits from\na regularization effect and linguistics infor-\nmation that leads to more general representa-\ntions to help adapt to new tasks and domains.\nLIMIT-BERT outperforms the strong baseline\nWhole Word Masking BERT on both depen-\ndency and constituent syntactic/semantic pars-\ning, GLUE benchmark, and SNLI task. Our\npractice on the proposed LIMIT-BERT also en-\nables us to release a well pre-trained model for\nmulti-purpose of natural language processing\ntasks once for all.\n1 Introduction\nRecently, pre-trained language models have shown\ngreatly effective across a range of linguistics in-\nspired natural language processing (NLP) tasks\nsuch as syntactic parsing, semantic parsing and\n*Corresponding author. This paper was partially sup-\nported by National Key Research and Development Program\nof China (No. 2017YFB0304100), Key Projects of Na-\ntional Natural Science Foundation of China (U1836222 and\n61733011), Huawei-SJTU long term AI project, Cutting-edge\nMachine reading comprehension and language model.\nso on (Zhou and Zhao, 2019; Zhou et al., 2020;\nOuchi et al., 2018; He et al., 2018b; Li et al., 2019),\nwhen taking the latter as downstream tasks for\nthe former. In the meantime, introducing linguis-\ntic clues such as syntax and semantics into the\npre-trained language models may furthermore en-\nhance other downstream tasks such as various Nat-\nural Language Understanding (NLU) tasks (Zhang\net al., 2020a,b). However, nearly all existing lan-\nguage models are usually trained on large amounts\nof unlabeled text data (Peters et al., 2018; Devlin\net al., 2019), without explicitly exploiting linguis-\ntic knowledge. Such observations motivate us to\njointly consider both types of tasks, pre-training\nlanguage models, and solving linguistics inspired\nNLP problems. We argue such a treatment may\nbenefit from two-fold. (1) Joint learning is a better\nway to let the former help the latter in a bidirec-\ntional mode, rather than in a unidirectional mode,\ntaking the latter as downstream tasks of the former.\n(2) Naturally empowered by linguistic clues from\njoint learning, pre-trained language models will be\nmore powerful for enhancing downstream tasks.\nThus we propose Linguistics Informed Multi-Task\nBERT (LIMIT-BERT), making an attempt to in-\ncorporate linguistic knowledge into pre-training\nlanguage representation models. The proposed\nLIMIT-BERT is implemented in terms of Multi-\nTask Learning (MTL) (Caruana, 1993) which has\nshown useful, by alleviating overfitting to a spe-\ncific task, thus making the learned representations\nuniversal across tasks.\nSince universal language representations are\nlearning by leveraging large amounts of unlabeled\ndata which has quite different data volume com-\npared with linguistics tasks dataset such as Penn\nTreebank (PTB)¹ (Marcus et al., 1993).\nTo alleviate such data unbalance on multi-task\n'PTB is an English treebank with syntactic tree annotation\nwhich only contains 50k sentences.\n\nPage 2:\nlearning, we apply semi-supervised learning ap-\nproach that uses a pre-trained linguistics model² to\nannotate large amounts of unlabeled text data and to\ncombine with gold linguistics task dataset as our fi-\nnal training data. For such pre-processing, it is easy\nto train our LIMIT-BERT on large amounts of data\nwith many tasks concurrently by simply summing\nup all the concerned losses together. Moreover,\nsince every sentence has labeled with predicted\nsyntax and semantics, we can furthermore improve\nthe masked training objective by fully exploiting\nthe known syntactic or semantic constituents dur-\ning the language model training process. Unlike\nthe previous work MT-DNN (Liu et al., 2019b)\nwhich only fine-tunes BERT on GLUE tasks, our\nLIMIT-BERT is trained on large amounts of data\nin a semi-supervised way and firmly supported by\nexplicit linguistic clues.\nWe verify the effectiveness and applicability\nof LIMIT-BERT on Propbank semantic parsing\n3 in both span style (CoNLL-2005) (Carreras and\nMàrquez, 2005) and dependency style, (CoNLL-\n2009) (Hajič et al., 2009) and Penn Treebank (PTB)\n(Marcus et al., 1993) for both constituent and de-\npendency syntactic parsing. Our empirical results\nshow that semantics and syntax can indeed ben-\nefit the language representation model via multi-\ntask learning and outperforms the strong baseline\nWhole Word Masking BERT (BERTWWM).\n2 Tasks and Datasets\nLIMIT-BERT includes five types of downstream\ntasks: Part-Of-Speech, constituent and dependency\nsyntactic parsing, span and dependency semantic\nrole labeling (SRL).\nBoth span (constituent) and dependency are two\nbroadly-adopted annotation styles for either seman-\ntics or syntax, which have been well studied and\ndiscussed from both linguistic and computational\nperspectives (Chomsky, 1981; Li et al., 2019).\nConstituency parsing aims to build a\nconstituency-based parse tree from a sentence\nthat represents its syntactic structure according to\na phrase structure grammar. While dependency\nparsing identifies syntactic relations (such as\nan adjective modifying a noun) between word\npairs in a sentence. The constituent structure\n2The model may jointly predict syntax and semantics for\nboth span and dependency annotation styles, which is from\n(Zhou et al., 2020) and joint learning with POS tag.\n3It is also called semantic role labeling (SRL) for the se-\nmantic parsing task over the Propbank.\nis better at disclosing phrasal continuity, while\nthe dependency structure is better at indicating\ndependency relation among words.\nSemantic role labeling (SRL) is dedicated to rec-\nognizing the predicate-argument structure of a sen-\ntence, such as who did what to whom, where and\nwhen, etc. For argument annotation, there are two\nformulizations. One is based on text spans, namely\nspan-based SRL. The other is dependency-based\nSRL, which annotates the syntactic head of argu-\nment rather than the entire argument span. SRL is\nan important method to obtain semantic informa-\ntion beneficial to a wide range of NLP tasks (Zhang\net al., 2019; Mihaylov and Frank, 2019).\nBERT is typically trained on quite large un-\nlabeled text datasets, BooksCorpus and English\nWikipedia, which have 13GB plain text, while the\ndatasets for specific linguistics tasks are less than\n100MB. Thus we employ semi-supervised learn-\ning to alleviate such data unbalance on multi-task\nlearning by using a pre-trained linguistics model\nto label BooksCorpus and English Wikipedia data.\nThe pre-trained model jointly learns POS tags and\nthe four types of structures on semantics and syn-\ntax, in which the latter is from the XLNet version\nof (Zhou et al., 2020), giving state-of-the-art or\ncomparable performance for the concerned four\nparsing tasks. During training, we set 10% proba-\nbility to use gold syntactic parsing and SRL data:\nPenn Treebank (PTB) (Marcus et al., 1993), span\nstyle SRL (CONLL-2005) (Carreras and Màrquez,\n2005) and dependency style SRL (CONLL-2009)\n(Hajič et al., 2009).\n2.1 Linguistics-Guided Mask Strategy\nBERT applies two training objectives: Masked\nLanguage Model (LM) and Next Sentence Predic-\ntion (NSP) based on WordPiece embeddings (Wu\net al., 2016) with a 30,000 token vocabulary. For\nMasked LM training objective, BERT uses training\ndata generator to choose 15% of the token posi-\ntions at random for mask replacement and predict\nthe masked tokens4. Since using different mask-\ning strategy can improve model performance such\nas the Whole Word Masking5 which masks all of\nthe tokens corresponding to a word at once, we\nfurther improve the masking strategy by exploit-\nActually, BERT applies three replacement strategies: (1)\nthe [MASK] token 80% of the time (2) random token 10% of\nthe time (3) the unchanged i-th token 10% of the time. This\nwork uses the same replacement strategies.\nShttps://github.com/huggingface/transformers\n\nPage 3:\nA0\nΑ1\nA0\nAl\nFederal Paper Board sells paper and wood products\nA2\nAl\nΑΠ\nSpan and Dependency SRL\nfederal paper board [MASK] paper and wood [MASK].\n(a) Semantic Phrase Masking.\nNP\n(1,3)\nS\n(1,9)\nVP\n(4,8)\nNNP NNP NNP\nFederal Paper Board\nVBZ\nNP\nsells\n(5,8)\n4\n2\n3\n5\nNN CC\npaper and\n6\nNN NNS\nwood products\n7\n8\nConstituent Syntactic Tree\n[MASK] [MASK] [MASK] sells paper and wood products.\n(b) Syntactic Phrase Masking.\nFigure 1: Syntactic and Semantic Phrase Masking strat-\negy. In figure (a) the predicates sells and products are\nreplaced by [MASK] while in figure (b) each token of\nconstituent federal paper board also has been masked.\ning available linguistic clues, syntactic or seman-\ntic constituents (phrases), predicted by our pre-\ntrained linguistics model as discussed in Section\n2. Thus, we apply three mask strategies at ran-\ndom for each sentence: Syntactic Phrase Masking,\nSemantic Phrase Masking, and Whole Word Mask-\ning. Syntactic/Semantic Phrase Masking (SPM)\nmeans that all the tokens corresponding to a syntac-\ntic/semantic phrase are masked, as shown in Fig-\nure 1. The overall masking rate and replacement\nstrategy remain the same as BERT, we still predict\neach masked WordPiece token independently. In-\ntuitively, it makes sense that SPM is strictly more\npowerful than original Token Masking or Whole\nWord Masking, since SPM may choose and pre-\ndict the meaningful words or phrases such as verb\npredicates or noun phrases.\n3 LIMIT-BERT Model\n3.1 Overview\nThe architecture of the LIMIT-BERT is shown in\nFigure 2. Our model includes four modules: to-\nken representation, Transformer encoder, language\nmodeling layers, task-specific layers including syn-\n6Syntactic phrases indicate the constituent subtrees while\nsemantic phrases represent as predicate or argument in span\nSRL.\ntactic and semantic scorers and decoders. We take\nmulti-task learning (MTL) (Caruana, 1993) sharing\nthe parameters of token representation and Trans-\nformer encoder, while language modeling layers\nand the top task-specific layers have independent\nparameters. The training procedure is simple that\nwe just sum up the language model loss with task-\nspecific losses together.\n3.2 Token Representation\nFollowing BERT token representation (Devlin\net al., 2019), the first token is always the [CLS]\ntoken. If input X is packed by a sentence pair\nX1; X2, we separate the two sentences with a spe-\ncial token [SEP] (\"packed by\" means connect two\nsentences as BERT training). The Transformer en-\ncoder maps\nX into a sequence of input embedding\nvectors, one for each token, which is a sum of the\ncorresponding word, segment, and positional em-\nbeddings.\nIf we apply BERT training data (BooksCorpus\nand English Wikipedia), we use pair sentences\npacked to perform next sentence prediction and\nonly take the first sentence including [CLS] and\n[SEP] token for later linguistics tasks. While using\ngold linguistics task data (PTB, CONLL-2005, and\nCONLL-2009) with 10% probability, we only take\none sentence as input that [CLS] and [SEP] are first\nand last tokens respectively.\nSince input sequence X is based on WordPiece\ntoken, we only take the last WordPiece vector of\nthe word in the last layer of Transformer encoder\nas our sole word representation for later linguistics\ntasks input to keep the same length of the token\nand label annotations.\n3.3 Transformer Encoder\nThe Transformer encoder in our model is adapted\nfrom (Vaswani et al.), which transforms the input\nrepresentation vectors into a sequence of contextu-\nalized embedding vectors with shared representa-\ntion across different tasks. We use the pre-trained\nparameters of BERT (Devlin et al., 2019) as our\nencoder initialization for faster convergence.\n3.4 Language Modeling Layers\nBERT training applies masked language modeling\n(MLM) as a training objective which corrupts the\ninput by replacing some tokens with a special token\n[MASK] and then lets the model reconstruct the\noriginal tokens. While in our LIMIT-BERT train-\ning, the linguistics specific tasks and MLM train-\n\nPage 4:\n[CLS]\n[CLS]\nROOT\n[Categ<S\nHEAD sells\n(1,9)\nPart-Of-Speech Score\n[Categ<NP>\nHEAD Board\n(1,3)\nNNP NNP NNP\nFederal Paper Board\n2\n[Categ<VP>]\nHEAD sells\n(4,8)\nVBZ [Categ NP>\nsells HEAD products\n4\n(5,8)\n[<#>\n[Categ aper\nHEAD\n(5,7)\nFederal\nFederal\nDependency Head Score\nPaper\nPaper\n[MASK]\n[SEP]\n[MASK]\n+\nproducts\nNNS\n|\nproducts\n⑩\nConstituent Span Score\nNN\npaper\nCC NN\nand wood\nLabel scorel\n\\Joint Span Structure\n6\nBroadcast\n[SEP]\nPredicate score[\nSound\nfamiliar\nfamiliar\n[SEP]\n[SEP]\nArgument score\nSemantic Role Score\nTask Specific Layer\nSoftmax\nA0\nΑΓ\nAl\nFederal Paper Board sells paper and wood products\nA0\nSpan and Dependency SRL\nDecoder Layer\nMasked Token Prediction\nReplaced Token Detection\nTask-specific Loss\nLanguage Model Loss\nAl\nToken Generator Token Discriminator\nRepresentation Transformer Representation Transformer\nLanguage Modeling Layer\nFigure 2: The framework of LIMIT-BERT.\ning take the same input; thus the [MASK] tokens\nraise a mismatch problem that the model sees artifi-\ncial [MASK] tokens during MLM training but not\nwhen being fine-tuned and inference on linguistics\ntasks. Besides, due to learning bidirectional rep-\nresentations, MLM approaches incur a substantial\ncomputational cost increase because the network\nonly learns from 15% of the tokens per example\nand needs more training time to converge.\nRecently (Yang et al., 2019; Clark et al., 2020)\nhave made attempts to alleviate such a difficulty.\nThe latter applies a replaced token detection task\nin their ELECTRA model. Instead of masking the\ninput, ELECTRA corrupts it by replacing some\ninput tokens with plausible alternatives sampled\nfrom a small generator network, which is close to\nthe original input without [MASK] tokens.\nWe adopt the ELECTRA training approach in\nour LIMIT-BERT, which lets the generator G and\ndiscriminator D share the same parameters and\nembedding as shown in Figure 2. The generator G\nis identical to BERT training (Devlin et al., 2019)\nthat predicts the masked tokens and next sentence\nand sums token mask loss and next sentence predict\nloss as JG(). Then the discriminator D takes the\nIf using gold linguistics task data, we only compute the\ntoken mask loss.\nLoss Calculation\npredicted tokens by generator G8 and is trained\nto distinguish tokens that have been replaced by\ngenerator G which is a simple binary classification\nof each token with loss JD(0). At last, we take\nthe output vector X of discriminator D to feed the\nfollowing task-specific layers and sum the loss of\nJG (0) and JD(0) as the final language modeling\nloss Jim (0):\nJim (0) = JG (0) + \\JD(0),\nwhere is set to 50 as the same as ELECTRA.\n3.5 Task-specific Layers\nFirstly, we rebuild word representations from the\nWordPiece tokens for linguistics tasks. Then we\nfollow (Zhou et al., 2020) to construct the task-\nspecific layers, including scoring layer and decoder\nlayer. The former scores three types of linguistic\nobjectives, dependency head, syntactic constituent\nand semantic role. The latter is to generate the legal\nlinguistics structures.\nWord Level Construction Suppose that X is\nthe output of the discriminator Transformer en-\ncoder, we pre-process the WordPiece sequence vec-\ntor X for linguistics specific tasks learning which\nFor the non-masked tokens, we take the original tokens\nas input.\n\nPage 5:\nare based on word level. We only take the first\nsentence X₁ including token [CLS] and [SEP] of\npacked sentence pair (X1; X2). Then we convert\nWordPiece sequence vector to word-level by sim-\nply taking the last WordPiece token vector of the\nword as the representation of the whole word.\nScoring Layer\nAfter the word-level construction, we calculate\nthe POS tag, syntactic constituent, dependency\nhead, and semantic role scores, following the train-\ning way as (Zhou et al., 2020) to construct syn-\ntactic constituent, dependency head, and semantic\nrole scores objective loss which are represented as\nJ1(0), J2(0) and J3 (0) respectively.\nFor POS tagging model training, we apply a\none-layer feedforward network and minimize the\nnegative log-likelihood of the gold POS tag gpi\nof each word, which is implemented as a cross-\nentropy loss:\nJ4(0) = -log Pe(gpi|xi),\nwhere x is word vector inside X.\nUtilizing these specific task scores, we do a sum\nto obtain the linguistics task loss Jit (0) for training:\nJit(0) = J1(0) + J2(0) + J3(0) + J4(0).\nAt last, our LIMIT-BERT is trained for simply min-\nimizing the overall loss:\nJoverall (0) = Jim(0) + Jit(0).\nDecoder Layer For syntactic parsing, we apply\nthe joint span CKY-style algorithm to generate\nconstituent and dependency syntactic tree simulta-\nneously by following (Zhou and Zhao, 2019).\nFor span and dependency SRL, we use a sin-\ngle dynamic programming decoder according to\nthe uniform semantic role score following the non-\noverlapping constraints: span semantic arguments\nfor the same predicate do not overlap (Punyakanok\net al., 2008). For further details of the scoring and\ndecoder layer, please refer to (Zhou et al., 2020).\n4 Experiments\n4.1 Evaluation\nWe use the model of (Zhou et al., 2020) with fine-\ntuned uncased BERTWWM (whole word masking)\nas the baseline. For fairly compared to the baseline\n⁹Our codes and the pre-trained models\nhttps://github.com/DoodleJZ/LIMIT-BERT.\n:\nBERTWWM, we also extract the language modeling\nlayer of LIMIT-BERT and use the same model of\n(Zhou et al., 2020) to fine-tune. We evaluate our\nproposed model LIMIT-BERT and baseline model\nBERTWWM on CoNLL-2009 shared task (Hajič\net al., 2009) for dependency-style SRL, CONLL-\n2005 shared task (Carreras and Màrquez, 2005)\nfor span-style SRL both using the Propbank con-\nvention (Palmer et al., 2005), and English Penn\nTreebank (PTB) (Marcus et al., 1993) for con-\nstituent syntactic parsing, Stanford basic dependen-\ncies (SD) representation (de Marneffe et al., 2006)\nconverted by the Stanford parser 10 for dependency\nsyntactic parsing using the same model of (Zhou\net al., 2020) to fine-tune. We follow standard data\nsplitting and evaluate setting as (Zhou et al., 2020)\nand use end-to-end SRL setups of both span and\ndependency SRL. Since LIMIT-BERT involves all\nsyntactic and semantic parsing tasks, it is possible\nto directly apply LIMIT-BERT to each task without\nfine-tuning and we also compare these results.\nIn order to evaluate the language model pre-\ntraining performance of our LIMIT-BERT, we\nalso evaluate LIMIT-BERT on two widely-used\ndatasets, The General Language Understanding\nEvaluation (GLUE) benchmark (Wang et al., 2018)\nwhich is a collection of nine NLU tasks and Stan-\nford Natural Language Inference (SNLI) (Bowman\net al., 2015) to show the superiority.\n4.2 Implementation Details\nOur implementation of LIMIT-BERT is based on\nthe PyTorch implementation of BERT¹¹. We use\na learning rate of 3e-5 and a batch size of 32 with\n1 million training steps. The optimizer and other\ntraining settings are the same as BERT (Devlin\net al., 2019). For task-specific layers including\nsyntactic and semantic scorers and decoders, we\nset the same hyperparameters settings as (Zhou\net al., 2020). LIMIT-BERT model is trained on 32\nNVIDIA GeForce GTX 1080Ti GPUs.\n4.3 Main Results\nSyntactic Parsing Results As shown in Table\n1 and 2, LIMIT-BERT without fine-tuning obtains\n95.84 F1 score of constituent parsing and 97.14%\nUAS and 95.44% LAS of dependency parsing.\nCompared with baseline BERTW LIMIT-BERT\nWWM,\n10http://nlp.stanford.edu/software/lex-parser.html\n\"https://github.com/huggingface/pytorch-pretrained-\nBERT. We use Whole Word Masking BERT parameters as\nour Transformer encoder initialization.\n\nPage 6:\nDozat and Manning (2017)\nMa et al. (2018)\nJi et al. (2019)\nLiu et al. (2019a)\nFernández-González and Gómez-Rodríguez (2019)\n96.09 95.03\nZhou and Zhao (2019)(BERT)\nZhou et al. (2020)(BERT)\nZhou et al. (2020)(XLNet)\nBaseline (BERTWWM)\nOur LIMIT-BERT\nOur LIMIT-BERT+\nUAS LAS\nModel\nDev\nTest\n95.74 94.08\n95.87 94.19\nDRCN (Kim et al., 2018)\n90.1\n95.97 94.31\nSJRC (Zhang et al., 2019)\n91.3\n96.04 94.43\nMT-DNN (Liu et al., 2019b)\n92.2\n91.6\n97.00 95.43\nSemBERT (Zhang et al., 2020a)\n91.9\n96.90 95.32\n97.23 95.65\nBaseline (BERTWWM)\n91.7\n91.4\n96.89 95.22\nLIMIT-BERT\n92.3\n91.7\n96.94 95.30\n97.14 95.44\nTable 1: Dependency syntactic parsing on PTB, no\nfinetuning result is marked by t.\nGaddy et al. (2018)\nKitaev and Klein (2018) (ELMO)\nKitaev et al. (2019)(BERT)\nZhou and Zhao (2019)(BERT)\nZhou et al. (2020)(BERT)\nZhou et al. (2020)(XLNet)\nBaseline (BERTWWM)\nOur LIMIT-BERT\nOur LIMIT-BERT+\nLR LP\nF1\n91.76 92.41 92.08\n94.85 95.40 95.13\n95.46 95.73 95.59\n95.70 95.98 95.84\n95.39 95.64 95.52\n96.10 96.26 96.18\n95.59 95.86 95.72\n95.67 95.92 95.80\n95.72 95.96 95.84\nTable 2: Constituent syntactic parsing on PTB, no fine-\ntuning result is marked by f.\nWSJ\nBrown\nSystem\nP\nR\nF1\nP\nR\nF1\nEnd-to-end Span SRL\nHe et al. (2018a)\nHe et al. (2018a)(ELMo)\nLi et al. (2019)(ELMO)\nStrubell et al. (2018) (ELMO)\nZhou et al. (2020)(BERT)\nZhou et al. (2020)(XLNet)\nBaseline (BERTWWM)\nOurs LIMIT-BERT\nOurs LIMIT-BERT+\nEnd-to-end Dependency SRL\nLi et al. (2019)\nHe et al. (2018b)\nCai et al. (2018)\nLi et al. (2019)(ELMO)\nZhou et al. (2020)(BERT)\nZhou et al. (2020)(XLNet)\nBaseline (BERTWWM)\nOurs LIMIT-BERT\nOurs LIMIT-BERT+\n81.2 83.9 82.5 69.7 71.9 70.8\n84.8 87.2 86.0 73.9 78.4 76.1\n85.2 87.5 86.3 74.7 78.1 76.4\n87.13 86.67 86.90 79.02 77.49 78.25\n86.46 88.23 87.34 77.26 80.20 78.70\n87.48 89.51 88.48 80.46 84.15 82.26\n86.48 88.59 87.52 79.4 82.68 81.01\n86.62 89.12 87.85 79.58 83.05 81.28\n87.16 88.51 87.83 79.20 80.29 79.74\n85.1\n83.9 82.7 83.3\n84.7 85.2 85.0\n73.8\n72.5\n74.2\n84.5 86.1 85.3 74.6\n86.77 89.14 87.94 79.71 82.40 81.03\n86.35 90.16 88.21 80.90 85.38 83.08\n85.13 89.21 87.12 79.05 83.95 81.43\n85.84 90.01 87.87 79.50 84.85 82.09\n85.73 89.34 87.50 79.60 82.81 81.17\nTable 3: Span SRL and dependency SRL results on on\nCONLL-2005 and CoNLL-2009 test sets in end-to-end\nmode, no finetuning result is marked by †.\noutperforms the baseline model both of fine-tuning\nor not. Particularly, LIMIT-BERT without fine-\ntuning exceeds more than 0.2 in UAS of depen-\ndency and 0.1 F1 of constituent syntactic pars-\ning which are considerable improvements on such\nstrong baselines.\nSemantic Parsing Results Table 3 shows re-\nsults on CoNLL-2005, CONLL-2009 in-domain\n(WSJ) and out-domain (Brown) test sets and com-\npares our LIMIT-BERT with previous published\nTable 4: Leaderboards of SNLI dataset. Both our\nLIMIT-BERT and BERTWWM are single models.\nstate-of-the-art models in end-to-end mode. The\nupper part of the table presents results from span\nSRL while the lower part shows results of depen-\ndency SRL. Compared with baseline, LIMIT-BERT\nwith fine-tuning outperforms BERTWWM on all\nfour SRL datasets, exceeding more than 0.3 in F1\nof in-domain span SRL and 0.7 F1 of dependency\nSRL, which demonstrate that LIMIT-BERT can\nfurthermore improve SRL performance even over\nstrong baselines.\nThe results of syntactic and semantic parsing\nempirically illustrate that incorporating linguis-\ntic knowledge into pre-trained language model by\nmulti-task and semi-supervised learning can signif-\nicantly enhance downstream tasks.\nSNLI Results\nTable 4 includes the best results\nreported in the leaderboards 12 of SNLI. We see\nthat LIMIT-BERT outperforms the strong baseline\nmodel BERTwwм in 0.3 F1 score on the SNLI\nbenchmark.\nGLUE Results We fine-tuned LIMIT-BERT for\neach GLUE task on task-specific data. The dev\nresults in Table 5 show that LIMIT-BERT outper-\nforms the strong baseline model and achieves re-\nmarkable results compared to other state-of-the-art\nmodels in literature.\n4.4 Discussions\nAblation Study\nLIMIT-BERT contains three\nkey components: Multi-Task learning, ELECTRA\ntraining approach, and Syntactic/Semantic Phrase\nMasking (SPM). To evaluate the contribution of\neach component in LIMIT-BERT, we remove each\ncomponent from the model for training and then\nfine-tune on downstream NLU tasks and linguistics\ntasks for evaluation. In consideration of compu-\ntational cost, we apply BERT base as the start of\ntraining and only use one-tenth of the BERT train-\ning corpus. We employ the same training setting\n12 https://nlp.stanford.edu/projects/\nsnli/\n\nPage 7:\nModel\nCOLA SST-2\n(mc) (acc)\nMRPC STS-B QQP\n(F1/acc) (pc/sc) (acc/F1)\nDev set results for Comparison\nMNLI QNLI RTE Score\nm/mm(acc) (acc) (acc)\nBERT\n60.6 93.2\nMT-DNN\nELECTRA\nBaseline (BERTWWM)\n63.5 94.3\n69.3\n96.0\n63.6 93.6\nLIMIT-BERT\n64.0 94.0\n-/88.0 -/90.0 91.3/-\n91.0/87.5 90.7/90.6 91.9/89.2\n-/90.6 -/92.1 92.4/-\n90.8/87.0 90.5/90.2 91.7/88.8\n94.0/91.7 91.5/91.3 91.6/88.6 87.4/87.3\nTest set results for models with standard single-task finetuning\n93.5\n-/86.6\n92.3 70.4 84.0\n87.1/86.7 92.9 83.4\n-/90.5 94.5 86.8 89.0\n87.4/87.2 93.9 77.3 85.6\n85.2 87.3\nBILSTM+ELMO+Attn 36.0 90.4 84.9/77.9 75.1/73.3 84.7/64.8 76.4/76.1\n56.8\n70.5\nBERT\nMT-DNN\nSemBERT\nELECTRA\n71.7\nLIMIT-BERT\n62.5\n94.5\n60.5\n94.9 89.3/85.4 87.6/86.5 89.3/72.1\n62.5 95.6 91.1/88.2 89.5/88.8 89.6/72.7\n62.3 94.6 91.2/88.3 87.8/86.7 89.8/72.8\n97.1 93.1/90.7 92.9/92.5 90.8/75.6\n90.9/88.0 90.3/89.7 89.5/71.9\n86.7/85.9 92.7 70.1\n86.7/86.0 93.1 81.4\n87.6/86.3 94.6 84.5\n82.9\n91.3/90.8 95.8 89.8 89.35\n87.1/86.2 94.0 83.0 83.3\n80.5\n82.7\nTable 5: Comparison of GLUE dev and test sets. Our model is in boldface. MT-DNN dev results are from (Liu\net al., 2019b) and other dev results are from (Clark et al., 2020).\nSystem\nDev\nLIMIT-BERT\n82.6\nGLUE SNLI SNLI\nDev Test\n90.6 91.0\nSEM span SEM dep SYNcon\nSYN dep\nSystem\nF1\nF1\nw/o Multi-Task 82.9\nw/o ELECTRA 81.3\nw/o SPM\n90.5 90.7\nBaseline\nLIMIT-BERT 87.13\nLIMIT-BERT+ 87.04\n86.55\n86.10\n86.77\n86.38\nF1\n95.52 96.54 94.71\n95.55 96.54 94.74\n95.72 96.82 94.82\nUAS LAS\n90.4 90.5\n80.2\n90.6 90.8\nTable 6:\nGLUE and SNLI.\nAblation study of LIMIT-BERT (base) on\nSEM span SEM dep\nSystem\nF1\nF1\nLIMIT-BERT\n86.25\n85.74\nSYN con\nF1\n95.34\nw/o Multi-Task\n85.60\n85.24\nw/o ELECTRA\n86.20\n85.71\nw/o SPM\n86.21\n85.72\nSYN dep\nUAS LAS\n96.59 94.71\n95.16 96.38 94.38\n95.32 96.59 94.70\n95.44 96.64 94.70\nTable 7: Ablation study of LIMIT-BERT (base) on lin-\nguistics tasks.\nfor each ablation model: 200k training steps, le-5\nlearning rate and 32 batch size. After language\nmodel training, we extract the layers of BERT base\nand fine-tune on downstream tasks for evaluation.\nThe ablation study is conducted on NLU tasks\nand linguistics tasks shown in Table 6 and 7 respec-\ntively. For NLU tasks, GLUE and SNLI results\nboth decrease in w/o ELECTRA and w/o SPM set-\nting showing the effectiveness of the ELECTRA\ntraining approach and SPM for NLU tasks. For\nlinguistics tasks, w/o Multi-Task setting hurts the\nperformance obviously from LIMIT-BERT both of\nsemantic and syntactic parsing, which shows the\neffectiveness of multi-tasks learning for linguistics\ntasks. Besides, the ELECTRA training approach\nTable 8: Fine-tuning effect analysis on English dev sets,\nno finetuning result is marked by †.\nand SPM also can improve performance when fine-\ntuning on linguistics tasks.\nComparing the results in Tables 6 and 7, ELEC-\nTRA training approach and SPM are more effec-\ntive for NLU tasks while multi-tasks learning can\nimprove the linguistics tasks performance signifi-\ncantly. The possible explanation is that multi-tasks\nlearning enables LIMIT-BERT to 'remember' the\nlinguistics information and thus lead to better per-\nformance in downstream linguistics tasks.\nFine-tuning Effect We examine the fine-tuning\neffect of LIMIT-BERT on linguistics tasks. The\nresults in Table 8 show that LIMIT-BERT with\nor without finetuning still outperforms BERTWWM\nbaseline consistently among all tasks. In such a\ncase, fine-tuning is necessary to boost the seman-\ntic parsing performance while no-fine-tuning per-\nforms better on syntactic parsing. As shown in Ta-\nble 8, the accuracy improves 0.1 F1 and 0.4 F1 of\nspan SRL and dependency SRL after fine-tuning re-\nspectively but no-fine-tuning performs better nearly\n0.2 F1 of syntactic parsing. The possible explana-\ntion is that no-fine-tuning LIMIT-BERT use semi-\nsupervised training data which contains much more\nlong sentence samples and benefits syntactic pars-\ning more.\n\nPage 8:\n97.59\nModel\nTest\nYasunaga et al. (2018)\nAkbik et al. (2018)\nBohnet et al. (2018)\nLIMIT-BERT\n97.85\n97.96\n97.71\nTable 9: POS tagging accuracy on the WSJ test set,\nwith other top-performing systems.\nF1 of Constituent Parsing\nF1 of Span SRL\n100\n90\n81\n85\n0-9\n78\n0-9\nBaseline Constituent\nLimitBert Constituent\nBaseline Dependency\nLimitBert Dependency\n100\ntotal\n10-19 20-29 30-39 40-49 50-59 60-69\nSyntactic Parsing of Different Sentences Length\nBaseline Span\nLimitBert Span\nBaseline Dependency\nLimitBert Dependency\n60-69 total\n10-19 20-29 30-39 40-49 50-59\nSRL of Different Sentences Length\n95\n90\n85\n80\n75\n90\n78\nUAS of Dependency Parsing\nF1 of Dependency SRL\nFigure 3: The performance of baseline model and\nLIMIT-BERT while varying sentence length of four lin-\nguistics tasks on the English dev set.\nPart-Of-Speech Performance\nTable 9 lists the\nresults of POS tagging on WSJ test set showing\nthat our LIMIT-BERT achieves competitive results\ncompared with other state-of-the-art models. Note\nthat we only apply simple one-layer decoder with-\nout those complicated ones such as conditional\nrandom field (CRF) (Lafferty et al., 2001) as the\nPOS tagging task is not the main concern of our\nmodel.\nSentences Length Performance\nThe perfor-\nmance of baseline model and LIMIT-BERT while\nvarying the sentence length of four linguistics tasks\non the English dev set is shown in Figure 3. The\nstatistics show that our LIMIT-BERT outperforms\nthe baseline model of over all sentence lengths. For\ndifferent sentence lengths, LIMIT-BERT outper-\nforms much better than baseline model on long sen-\ntence (larger than 50) of both syntactic and seman-\ntic parsing. The possible explanation is that LIMIT-\nBERT uses semi-supervised training data, which\ncontains much more long sentence samples and\nbenefits parsing performance on long sentences.\n5 Related Work\nLinguistics Inspired NLP Tasks With the im-\npressive success of deep neural networks in various\nNLP tasks (Chen and Manning, 2014; Dozat and\nManning, 2017; Ma et al., 2018; Strubell et al.,\n2018; Luo and Zhao, 2020; Li et al., 2020; He\net al., 2019; Luo et al., 2020; Zhang et al., 2018a;\nLi et al., 2018a; Zhang et al., 2018b), syntactic\nparsing and semantic role labeling have been well\ndeveloped with neural network and achieve very\nhigh performance (Chen and Manning, 2014; Dozat\nand Manning, 2017; Ma et al., 2018; Kitaev and\nKlein, 2018; Zhou and Zhao, 2019). Semantic\nrole labeling is deeply related to syntactic structure\nand a number of works try to incorporate syntac-\ntic information in semantic role labeling models\nby different methods such as concatenation of lex-\nicalized embedding (He et al., 2018b), usage of\nsyntactic GCN (Li et al., 2018b) and multi-task\nlearning (Strubell et al., 2018; Zhou et al., 2020).\nBesides semantic role labeling and syntactic pars-\ning are two key tasks of semantics and syntax so\nthat they are included into our linguistics tasks for\nmulti-task learning.\nIn addition, both span and dependency are pop-\nularly adopted annotation styles for both seman-\ntics and syntax and some work on jointly learn-\ning of semantic and syntactic (Henderson et al.,\n2013; Lluís et al., 2013; Swayamdipta et al., 2016)\n.\nResearchers are interested in two styles of\nSRL models that may benefit from each other\nrather than their separated development, which\nhas been roughly discussed in (Johansson and\nNugues, 2008). On the other hand, researchers\nhave discussed how to encode lexical dependencies\nin phrase structures, like lexicalized tree adjoining\ngrammar (LTAG) (Schabes et al., 1988), Combina-\ntory Categorial Grammar (CCG) (Steedman, 2000)\nand head-driven phrase structure grammar (HPSG)\n(Pollard and Sag, 1994) which is a constraint-\nbased highly lexicalized non-derivational genera-\ntive grammar framework. To absorb both strengths\nof\nspan and dependency structure, we apply both\nspan (constituent) and dependency representations\nof semantic role labeling and syntactic parsing.\nThus, it is a natural idea to study the relationship be-\ntween constituent and dependency structures, and\n\nPage 9:\nthe joint learning of constituent and dependency\nsyntactic parsing (Klein and Manning, 2004; Char-\nniak and Johnson, 2005; Farkas et al., 2011; Green\nand Žabokrtský, 2012; Ren et al., 2013; Xu et al.,\n2014; Yoshikawa et al., 2017).\nPre-trained Language Modeling\nRecently,\ndeep contextual language model has been shown\neffective for learning universal language represen-\ntations by leveraging large amounts of unlabeled\ndata, achieving various state-of-the-art results in\na series of NLU benchmarks. Some prominent\nexamples are Embedding from Language models\n(ELMO) (Peters et al., 2018), Bidirectional Encoder\nRepresentations from Transformers (BERT) (De-\nvlin et al., 2019) and Generalized Autoregressive\nPretraining (XLNet) (Yang et al., 2019).\nMany latest works make attempts to modify the\nlanguage model based on BERT such as ELEC-\nTRA (Clark et al., 2020) and MT-DNN (Liu et al.,\n2019b). ELECTRA focuses on the [MASK] tokens\nmismatch problem and thus combines the idea of\nGenerative Adversarial Networks GANS (Good-\nfellow et al.). MT-DNN applies multi-task learn-\ning to language model pre-training and achieves\nnew state-of-the-art results on GLUE benchmark.\nBesides, (Gururangan et al., 2020) finds that mul-\ntiphase adaptive pretraining offers large gains in\ntask performance which is similar with our semi-\nsupervised learning strategy.\n6 Conclusions\nIn this work, we present LIMIT-BERT which ap-\nplies multi-task learning with multiple linguistic\ntasks by semi-supervised learning. We use five\nkey syntax and semantics tasks: Part-Of-Speech\n(POS) tags, constituent and dependency syntactic\nparsing, span and dependency semantic role label-\ning (SRL). and further improve the masking strat-\negy of BERT training by effectively exploiting the\navailable syntactic and semantic clues for language\nmodel training. The experiments show that LIMIT-\nBERT outperforms the strong baseline BERTwWM\non four benchmark parsing treebanks and two NLU\ntasks. The results of GLUE and SNLI empirically\nillustrate that incorporating linguistic knowledge\ninto pre-training language BERT by multi-task and\nsemi-supervised learning can also enhance down-\nstream tasks. There are many future areas to ex-\nplore to improve LIMIT-BERT, including a deeper\nunderstanding of model structure sharing in MTL, a\nmore effective training method that leverages relat-\nedness among multiple tasks, for both fine-tuning\nand pre-training, and ways of incorporating the\nlinguistic structure of text in a more explicit and\ncontrollable manner.\nReferences\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nlabeling. In Proceedings of the 27th International\nConference on Computational Linguistics, pages\n1638-1649, Santa Fe, New Mexico, USA. Associ-\nation for Computational Linguistics.\nBernd Bohnet, Ryan McDonald, Gonçalo Simões,\nDaniel Andor, Emily Pitler, and Joshua Maynez.\n2018. Morphosyntactic tagging with a meta-\nBILSTM model over context sensitive token encod-\nings. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 2642-2652, Melbourne,\nAustralia. Association for Computational Linguis-\ntics.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632-642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nJiaxun Cai, Shexia He, Zuchao Li, and Hai Zhao. 2018.\nA full end-to-end semantic role labeler, syntactic-\nagnostic over syntactic-aware? In Proceedings of\nthe 27th International Conference on Computational\nLinguistics, pages 2753-2765, Santa Fe, New Mex-\nico, USA. Association for Computational Linguis-\ntics.\nXavier Carreras and Lluís Màrquez. 2005. Introduc-\ntion to the CoNLL-2005 shared task: Semantic\nrole labeling. In Proceedings of the Ninth Confer-\nence on Computational Natural Language Learning\n(CONLL-2005), pages 152-164, Ann Arbor, Michi-\ngan. Association for Computational Linguistics.\nRichard A Caruana. 1993. Multitask Learning: A\nKnowledge-Based Source of Inductive Bias. In Ma-\nchine Learning Proceedings.\nEugene Charniak and Mark Johnson. 2005. Coarse-\nto-Fine n-Best Parsing and MaxEnt Discriminative\nReranking. In Proceedings of the 43rd Annual Meet-\ning of the Association for Computational Linguistics\n(ACL).\nDanqi Chen and Christopher Manning. 2014. A Fast\nand Accurate Dependency Parser using Neural Net-\nworks. In Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP).\n\nPage 10:\nN. Chomsky. 1981. Lectures on Government and Bind-\ning. Mouton de Gruyter.\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and\nChristopher D. Manning. 2020. {ELECTRA}: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In International Conference on Learn-\ning Representations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies (NAACL:HLT).\nTimothy Dozat and Christopher D. Manning. 2017.\nDeep Biaffine Attention for Neural Dependency\nParsing. In International Conference on Learning\nRepresentations 2017 (ICLR).\nRichard Farkas, Bernd Bohnet, and Helmut Schmid.\n2011. Features for Phrase-Structure Reranking from\nDependency Parses. In Proceedings of the 12th In-\nternational Conference on Parsing Technologies.\nDaniel Fernández-González and Carlos Gómez-\nRodríguez. 2019. Left-to-Right Dependency\nParsing with Pointer Networks. In Proceedings of\nthe 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies (NAACL).\nDavid Gaddy, Mitchell Stern, and Dan Klein. 2018.\nWhat's Going On in Neural Constituency Parsers?\nAn Analysis. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies (NAACL: HLT).\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversar-\nial nets. In Advances in neural information process-\ning systems (NIPS).\nNathan Green and Zdeněk Žabokrtský. 2012.\nHy-\nbrid Combination of Constituency and Dependency\nTrees into an Ensemble Dependency Parser. In Pro-\nceedings of the Workshop on Innovative Hybrid Ap-\nproaches to the Processing of Textual Data.\nSuchin Gururangan, Ana Marasović, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don't Stop Pretraining:\nAdapt Language Models to Domains and Tasks.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics (ACL).\nJan Hajič, Massimiliano Ciaramita, Richard Johans-\nson, Daisuke Kawahara, Maria Antònia Martí, Lluís\nMàrquez, Adam Meyers, Joakim Nivre, Sebastian\nPadó, Jan Štěpánek, Pavel Straňák, Mihai Surdeanu,\nNianwen Xue, and Yi Zhang. 2009. The CoNLL-\n2009 shared task: Syntactic and semantic depen-\ndencies in multiple languages. In Proceedings of\nthe Thirteenth Conference on Computational Nat-\nural Language Learning (CoNLL 2009): Shared\nTask, pages 1-18, Boulder, Colorado. Association\nfor Computational Linguistics.\nLuheng He, Kenton Lee, Omer Levy, and Luke Zettle-\nmoyer. 2018a. Jointly predicting predicates and ar-\nguments in neural semantic role labeling. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers), pages 364-369, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nShexia He, Zuchao Li, and Hai Zhao. 2019. Syntax-\naware Multilingual Semantic Role Labeling. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP).\nShexia He, Zuchao Li, Hai Zhao, and Hongxiao Bai.\n2018b. Syntax for semantic role labeling, to be, or\nnot to be. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 2061-2071, Mel-\nbourne, Australia. Association for Computational\nLinguistics.\nJames Henderson, Paola Merlo, Ivan Titov, and\nGabriele Musillo. 2013. Multilingual joint pars-\ning of syntactic and semantic dependencies with a\nlatent variable model. Computational Linguistics,\n39(4):949-998.\nTao Ji, Yuanbin Wu, and Man Lan. 2019. Graph-\nbased dependency parsing with graph neural net-\nworks. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 2475-2485, Florence, Italy. Association\nfor Computational Linguistics.\nRichard Johansson and Pierre Nugues. 2008.\nDependency-based semantic role labeling of\nPropBank. In Proceedings of the 2008 Conference\non Empirical Methods in Natural Language Process-\ning, pages 69-78, Honolulu, Hawaii. Association\nfor Computational Linguistics.\nSeonhoon Kim, Jin-Hyuk Hong, Inho Kang, and No-\njun Kwak. 2018. Semantic sentence matching with\ndensely-connected recurrent and co-attentive infor-\nmation. arXiv preprint arXiv:1805.11360.\nNikita Kitaev, Steven Cao, and Dan Klein. 2019. Mul-\ntilingual Constituency Parsing with Self-Attention\nand Pre-Training. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics (ACL).\nNikita Kitaev and Dan Klein. 2018. Constituency Pars-\ning with a Self-Attentive Encoder. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (ACL).\n\nPage 11:\nDan Klein and Christopher Manning. 2004. Corpus-\nBased Induction of Syntactic Structure: Models of\nDependency and Constituency. In Proceedings of\nthe 42nd Annual Meeting of the Association for Com-\nputational Linguistics (ACL).\nJohn D. Lafferty, Andrew McCallum, and Fernando\nC. N. Pereira. 2001. Conditional Random Fields:\nProbabilistic Models for Segmenting and Label-\ning Sequence Data. In Proceedings of the Eigh-\nteenth International Conference on Machine Learn-\ning (ICML), pages 282-289.\nZuchao Li, Jiaxun Cai, Shexia He, and Hai Zhao.\n2018a. Seq2seq dependency parsing. In Proceed-\nings of the 27th International Conference on Com-\nputational Linguistics, pages 3203-3214, Santa Fe,\nNew Mexico, USA. Association for Computational\nLinguistics.\nZuchao Li, Shexia He, Jiaxun Cai, Zhuosheng Zhang,\nHai Zhao, Gongshen Liu, Linlin Li, and Luo Si.\n2018b. A unified syntax-aware framework for se-\nmantic role labeling. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2401-2411, Brussels, Bel-\ngium. Association for Computational Linguistics.\nZuchao Li, Shexia He, Hai Zhao, Yiqing Zhang, Zhu-\nosheng Zhang, Xi Zhou, and Xiang Zhou. 2019. De-\npendency or span, end-to-end uniform semantic role\nlabeling. In The Thirty-Third AAAI Conference on\nArtificial Intelligence (AAAI).\nZuchao Li, Rui Wang, Kehai Chen, Masso Utiyama,\nEiichiro Sumita, Zhuosheng Zhang, and Hai Zhao.\n2020. Data-dependent gaussian prior objective for\nlanguage generation. In International Conference\non Learning Representations.\nLinlin Liu, Xiang Lin, Shafiq Joty, Simeng Han, and Li-\ndong Bing. 2019a. Hierarchical Pointer Net Parsing.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP). Associa-\ntion for Computational Linguistics.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019b. Multi-task deep neural networks\nfor natural language understanding. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4487-4496, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nXavier Lluís, Xavier Carreras, and Lluís Màrquez.\n2013. Joint arc-factored parsing of syntactic and se-\nmantic dependencies. Transactions of the Associa-\ntion for Computational Linguistics, 1:219-230.\nYing Luo and Hai Zhao. 2020. Bipartite Flat-Graph\nNetwork for Nested Named Entity Recognition. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics (ACL).\nYing Luo, Hai Zhao, and Junlang Zhan. 2020. Named\nEntity Recognition Only from Word Embeddings.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\nXuezhe Ma, Zecong Hu, Jingzhou Liu, Nanyun Peng,\nGraham Neubig, and Eduard Hovy. 2018. Stack-\nPointer Networks for Dependency Parsing. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL).\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a Large Annotated\nCorpus of English: The Penn Treebank. Computa-\ntional Linguistics, 19(2).\nMarie-Catherine de Marneffe, Bill MacCartney, and\nChristopher D. Manning. 2006. Generating Typed\nDependency Parses from Phrase Structure Parses. In\nProceedings of the Fifth International Conference\non Language Resources and Evaluation (LREC’06).\nTodor Mihaylov and Anette Frank. 2019. Discourse-\nAware Semantic Self-Attention for Narrative Read-\ning Comprehension. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP).\nHiroki Ouchi, Hiroyuki Shindo, and Yuji Matsumoto.\n2018. A span selection model for semantic role la-\nbeling. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1630-1642, Brussels, Belgium. Association\nfor Computational Linguistics.\nMartha Palmer, Daniel Gildea, and Paul Kingsbury.\n2005. The proposition bank: An annotated cor-\npus of semantic roles. Computational Linguistics,\n31(1):71–106.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep Contextualized Word Rep-\nresentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies (NAACL: HLT).\nCarl Pollard and Ivan A Sag. 1994. Head-Driven\nPhrase Structure Grammar. University of Chicago\nPress.\nVasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.\nThe importance of syntactic parsing and inference in\nsemantic role labeling. Computational Linguistics,\n34(2).\nXiaona Ren, Xiao Chen, and Chunyu Kit. 2013.\nCombine Constituent and Dependency Parsing via\nReranking. In Proceedings of the Twenty-Third\nInternational Joint Conference on Artificial Intelli-\ngence (IJCAI).\n\nPage 12:\nYves Schabes, Anne Abeille, and Aravind K. Joshi.\n1988. Parsing strategies with 'lexicalized' gram-\nmars: Application to tree adjoining grammars. In\nColing Budapest 1988 Volume 2: International Con-\nference on Computational Linguistics (COLING).\nMark Steedman. 2000. The Syntactic Process. MIT\nPress, Cambridge, MA, USA.\nEmma Strubell,\nPatrick Verga, Daniel Andor,\nDavid Weiss, and Andrew McCallum. 2018.\nLinguistically-informed self-attention for semantic\nrole labeling. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 5027-5038, Brussels, Belgium.\nAssociation for Computational Linguistics.\nSwabha Swayamdipta, Miguel Ballesteros, Chris Dyer,\nand Noah A. Smith. 2016. Greedy, joint syntactic-\nsemantic parsing with stack LSTMs. In Proceedings\nof The 20th SIGNLL Conference on Computational\nNatural Language Learning, pages 187-197, Berlin,\nGermany. Association for Computational Linguis-\ntics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is All you\nNeed. In Advances in Neural Information Process-\ning Systems (NIPS).\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google's Neural Ma-\nchine Translation System: Bridging the Gap be-\ntween Human and Machine Translation. CORR,\nabs/1609.08144.\nWenduan Xu, Stephen Clark, and Yue Zhang. 2014.\nShift-Reduce CCG Parsing with a Dependency\nModel. In Proceedings of the 52nd Annual Meet-\ning of the Association for Computational Linguistics\n(ACL).\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Thirty-Third Annual\nConference on Neural Information Processing Sys-\ntems (NeurIPS).\nMichihiro Yasunaga, Jungo Kasai, and Dragomir\nRadev. 2018. Robust multilingual part-of-speech\ntagging via adversarial training. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 976-986, New Orleans, Louisiana. As-\nsociation for Computational Linguistics.\nMasashi Yoshikawa, Hiroshi Noji, and Yuji Matsumoto.\n2017. A* CCG Parsing with a Supertag and De-\npendency Factored Model. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (ACL).\nZhisong Zhang, Rui Wang, Masao Utiyama, Eiichiro\nSumita, and Hai Zhao. 2018a. Exploring recombina-\ntion for efficient decoding of neural machine trans-\nlation. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 4785-4790, Brussels, Belgium. Association\nfor Computational Linguistics.\nZhuosheng Zhang, Jiangtong Li, Pengfei Zhu, Hai\nZhao, and Gongshen Liu. 2018b. Modeling multi-\nturn conversation with deep utterance aggregation.\nIn Proceedings of the 27th International Conference\non Computational Linguistics, pages 3740-3752,\nSanta Fe, New Mexico, USA. Association for Com-\nputational Linguistics.\nZhuosheng Zhang, Yuwei Wu, Zuchao Li, and Hai\nZhao. 2019. Explicit contextual semantics for text\ncomprehension. In The 33rd Pacific Asia Confer-\nence on Language, Information and Computation\n(PACLIC).\nZhuosheng Zhang, Yuwei Wu, Hai Zhao, Zuchao Li,\nShuailiang Zhang, Xi Zhou, and Xiang Zhou. 2020a.\nSemantics-aware BERT for Language Understand-\ning. In The Thirty-Fourth AAAI Conference on Ar-\ntificial Intelligence (AAAI).\nZhuosheng Zhang, Yuwei Wu, Junru Zhou, Sufeng\nDuan, Hai Zhao, and Rui Wang. 2020b. SG-Net:\nSyntax-Guided Machine Reading Comprehension.\nIn The Thirty-Fourth AAAI Conference on Artificial\nIntelligence (AAAI).\nJunru Zhou, Zuchao Li, and Hai Zhao. 2020. Pars-\ning All: Syntax and Semantics, Dependencies and\nSpans. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nJunru Zhou and Hai Zhao. 2019. Head-driven phrase\nIn\nstructure grammar parsing on Penn treebank.\nProceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n2396-2408, Florence, Italy. Association for Compu-\ntational Linguistics.\n\n"
