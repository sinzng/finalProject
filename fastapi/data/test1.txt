extracted_text": "Page 1:\narXiv:1910.14296v2 [cs.CL] 6 Oct 2020\nLIMIT-BERT: Linguistics Informed Multi-Task BERT\n1,2,3\nJunru Zhou1,2,3, Zhuosheng Zhang 1,2,3, Hai Zhao1,2,3*, Shuailiang Zhang\n1\n¹Department of Computer Science and Engineering, Shanghai Jiao Tong University\n2 Key Laboratory of Shanghai Education Commission for Intelligent Interaction\nand Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China\n3 MOE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University\n{zhoujunru, zhangzs}@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn\nAbstract\nIn this paper, we present Linguistics Informed\nMulti-Task BERT (LIMIT-BERT) for learning\nlanguage representations across multiple lin-\nguistics tasks by Multi-Task Learning. LIMIT-\nBERT includes five key linguistics tasks: Part-\nOf-Speech (POS) tags, constituent and de-\npendency syntactic parsing, span and depen-\ndency semantic role labeling (SRL). Differ-\nent from recent Multi-Task Deep Neural Net-\nworks (MT-DNN), our LIMIT-BERT is fully\nlinguistics motivated and thus is capable of\nadopting an improved masked training objec-\ntive according to syntactic and semantic con-\nstituents. Besides, LIMIT-BERT takes a semi-\nsupervised learning strategy to offer the same\nlarge amount of linguistics task data as that\nfor the language model training. As a re-\nsult, LIMIT-BERT not only improves linguis-\ntics tasks performance, but also benefits from\na regularization effect and linguistics infor-\nmation that leads to more general representa-\ntions to help adapt to new tasks and domains.\nLIMIT-BERT outperforms the strong baseline\nWhole Word Masking BERT on both depen-\ndency and constituent syntactic/semantic pars-\ning, GLUE benchmark, and SNLI task. Our\npractice on the proposed LIMIT-BERT also en-\nables us to release a well pre-trained model for\nmulti-purpose of natural language processing\ntasks once for all.\n1 Introduction\nRecently, pre-trained language models have shown\ngreatly effective across a range of linguistics in-\nspired natural language processing (NLP) tasks\nsuch as syntactic parsing, semantic parsing and\n*Corresponding author. This paper was partially sup-\nported by National Key Research and Development Program\nof China (No. 2017YFB0304100), Key Projects of Na-\ntional Natural Science Foundation of China (U1836222 and\n61733011), Huawei-SJTU long term AI project, Cutting-edge\nMachine reading comprehension and language model.\nso on (Zhou and Zhao, 2019; Zhou et al., 2020;\nOuchi et al., 2018; He et al., 2018b; Li et al., 2019),\nwhen taking the latter as downstream tasks for\nthe former. In the meantime, introducing linguis-\ntic clues such as syntax and semantics into the\npre-trained language models may furthermore en-\nhance other downstream tasks such as various Nat-\nural Language Understanding (NLU) tasks (Zhang\net al., 2020a,b). However, nearly all existing lan-\nguage models are usually trained on large amounts\nof unlabeled text data (Peters et al., 2018; Devlin\net al., 2019), without explicitly exploiting linguis-\ntic knowledge. Such observations motivate us to\njointly consider both types of tasks, pre-training\nlanguage models, and solving linguistics inspired\nNLP problems. We argue such a treatment may\nbenefit from two-fold. (1) Joint learning is a better\nway to let the former help the latter in a bidirec-\ntional mode, rather than in a unidirectional mode,\ntaking the latter as downstream tasks of the former.\n(2) Naturally empowered by linguistic clues from\njoint learning, pre-trained language models will be\nmore powerful for enhancing downstream tasks.\nThus we propose Linguistics Informed Multi-Task\nBERT (LIMIT-BERT), making an attempt to in-\ncorporate linguistic knowledge into pre-training\nlanguage representation models. The proposed\nLIMIT-BERT is implemented in terms of Multi-\nTask Learning (MTL) (Caruana, 1993) which has\nshown useful, by alleviating overfitting to a spe-\ncific task, thus making the learned representations\nuniversal across tasks.\nSince universal language representations are\nlearning by leveraging large amounts of unlabeled\ndata which has quite different data volume com-\npared with linguistics tasks dataset such as Penn\nTreebank (PTB)¹ (Marcus et al., 1993).\nTo alleviate such data unbalance on multi-task\n'PTB is an English treebank with syntactic tree annotation\nwhich only contains 50k sentences.\n\nPage 2:\nlearning, we apply semi-supervised learning ap-\nproach that uses a pre-trained linguistics model² to\nannotate large amounts of unlabeled text data and to\ncombine with gold linguistics task dataset as our fi-\nnal training data. For such pre-processing, it is easy\nto train our LIMIT-BERT on large amounts of data\nwith many tasks concurrently by simply summing\nup all the concerned losses together. Moreover,\nsince every sentence has labeled with predicted\nsyntax and semantics, we can furthermore improve\nthe masked training objective by fully exploiting\nthe known syntactic or semantic constituents dur-\ning the language model training process. Unlike\nthe previous work MT-DNN (Liu et al., 2019b)\nwhich only fine-tunes BERT on GLUE tasks, our\nLIMIT-BERT is trained on large amounts of data\nin a semi-supervised way and firmly supported by\nexplicit linguistic clues.\nWe verify the effectiveness and applicability\nof LIMIT-BERT on Propbank semantic parsing\n3 in both span style (CoNLL-2005) (Carreras and\nMàrquez, 2005) and dependency style, (CoNLL-\n2009) (Hajič et al., 2009) and Penn Treebank (PTB)\n(Marcus et al., 1993) for both constituent and de-\npendency syntactic parsing. Our empirical results\nshow that semantics and syntax can indeed ben-\nefit the language representation model via multi-\ntask learning and outperforms the strong baseline\nWhole Word Masking BERT (BERTWWM).\n2 Tasks and Datasets\nLIMIT-BERT includes five types of downstream\ntasks: Part-Of-Speech, constituent and dependency\nsyntactic parsing, span and dependency semantic\nrole labeling (SRL).\nBoth span (constituent) and dependency are two\nbroadly-adopted annotation styles for either seman-\ntics or syntax, which have been well studied and\ndiscussed from both linguistic and computational\nperspectives (Chomsky, 1981; Li et al., 2019).\nConstituency parsing aims to build a\nconstituency-based parse tree from a sentence\nthat represents its syntactic structure according to\na phrase structure grammar. While dependency\nparsing identifies syntactic relations (such as\nan adjective modifying a noun) between word\npairs in a sentence. The constituent structure\n2The model may jointly predict syntax and semantics for\nboth span and dependency annotation styles, which is from\n(Zhou et al., 2020) and joint learning with POS tag.\n3It is also called semantic role labeling (SRL) for the se-\nmantic parsing task over the Propbank.\nis better at disclosing phrasal continuity, while\nthe dependency structure is better at indicating\ndependency relation among words.\nSemantic role labeling (SRL) is dedicated to rec-\nognizing the predicate-argument structure of a sen-\ntence, such as who did what to whom, where and\nwhen, etc. For argument annotation, there are two\nformulizations. One is based on text spans, namely\nspan-based SRL. The other is dependency-based\nSRL, which annotates the syntactic head of argu-\nment rather than the entire argument span. SRL is\nan important method to obtain semantic informa-\ntion beneficial to a wide range of NLP tasks (Zhang\net al., 2019; Mihaylov and Frank, 2019).\nBERT is typically trained on quite large un-\nlabeled text datasets, BooksCorpus and English\nWikipedia, which have 13GB plain text, while the\ndatasets for specific linguistics tasks are less than\n100MB. Thus we employ semi-supervised learn-\ning to alleviate such data unbalance on multi-task\nlearning by using a pre-trained linguistics model\nto label BooksCorpus and English Wikipedia data.\nThe pre-trained model jointly learns POS tags and\nthe four types of structures on semantics and syn-\ntax, in which the latter is from the XLNet version\nof (Zhou et al., 2020), giving state-of-the-art or\ncomparable performance for the concerned four\nparsing tasks. During training, we set 10% proba-\nbility to use gold syntactic parsing and SRL data:\nPenn Treebank (PTB) (Marcus et al., 1993), span\nstyle SRL (CONLL-2005) (Carreras and Màrquez,\n2005) and dependency style SRL (CONLL-2009)\n(Hajič et al., 2009).\n2.1 Linguistics-Guided Mask Strategy\nBERT applies two training objectives: Masked\nLanguage Model (LM) and Next Sentence Predic-\ntion (NSP) based on WordPiece embeddings (Wu\net al., 2016) with a 30,000 token vocabulary. For\nMasked LM training objective, BERT uses training\ndata generator to choose 15% of the token posi-\ntions at random for mask replacement and predict\nthe masked tokens4. Since using different mask-\ning strategy can improve model performance such\nas the Whole Word Masking5 which masks all of\nthe tokens corresponding to a word at once, we\nfurther improve the masking strategy by exploit-\nActually, BERT applies three replacement strategies: (1)\nthe [MASK] token 80% of the time (2) random token 10% of\nthe time (3) the unchanged i-th token 10% of the time. This\nwork uses the same replacement strategies.\nShttps://github.com/huggingface/transformers\n\nPage 3:\nA0\nΑ1\nA0\nAl\nFederal Paper Board sells paper and wood products\nA2\nAl\nΑΠ\nSpan and Dependency SRL\nfederal paper board [MASK] paper and wood [MASK].\n(a) Semantic Phrase Masking.\nNP\n(1,3)\nS\n(1,9)\nVP\n(4,8)\nNNP NNP NNP\nFederal Paper Board\nVBZ\nNP\nsells\n(5,8)\n4\n2\n3\n5\nNN CC\npaper and\n6\nNN NNS\nwood products\n7\n8\nConstituent Syntactic Tree\n[MASK] [MASK] [MASK] sells paper and wood products.\n(b) Syntactic Phrase Masking.\nFigure 1: Syntactic and Semantic Phrase Masking strat-\negy. In figure (a) the predicates sells and products are\nreplaced by [MASK] while in figure (b) each token of\nconstituent federal paper board also has been masked.\ning available linguistic clues, syntactic or seman-\ntic constituents (phrases), predicted by our pre-\ntrained linguistics model as discussed in Section\n2. Thus, we apply three mask strategies at ran-\ndom for each sentence: Syntactic Phrase Masking,\nSemantic Phrase Masking, and Whole Word Mask-\ning. Syntactic/Semantic Phrase Masking (SPM)\nmeans that all the tokens corresponding to a syntac-\ntic/semantic phrase are masked, as shown in Fig-\nure 1. The overall masking rate and replacement\nstrategy remain the same as BERT, we still predict\neach masked WordPiece token independently. In-\ntuitively, it makes sense that SPM is strictly more\npowerful than original Token Masking or Whole\nWord Masking, since SPM may choose and pre-\ndict the meaningful words or phrases such as verb\npredicates or noun phrases.\n3 LIMIT-BERT Model\n3.1 Overview\nThe architecture of the LIMIT-BERT is shown in\nFigure 2. Our model includes four modules: to-\nken representation, Transformer encoder, language\nmodeling layers, task-specific layers including syn-\n6Syntactic phrases indicate the constituent subtrees while\nsemantic phrases represent as predicate or argument in span\nSRL.\ntactic and semantic scorers and decoders. We take\nmulti-task learning (MTL) (Caruana, 1993) sharing\nthe parameters of token representation and Trans-\nformer encoder, while language modeling layers\nand the top task-specific layers have independent\nparameters. The training procedure is simple that\nwe just sum up the language model loss with task-\nspecific losses together.\n3.2 Token Representation\nFollowing BERT token representation (Devlin\net al., 2019), the first token is always the [CLS]\ntoken. If input X is packed by a sentence pair\nX1; X2, we separate the two sentences with a spe-\ncial token [SEP] (\"packed by\" means connect two\nsentences as BERT training). The Transformer en-\ncoder maps\nX into a sequence of input embedding\nvectors, one for each token, which is a sum of the\ncorresponding word, segment, and positional em-\nbeddings.\nIf we apply BERT training data (BooksCorpus\nand English Wikipedia), we use pair sentences\npacked to perform next sentence prediction and\nonly take the first sentence including [CLS] and\n[SEP] token for later linguistics tasks. While using\ngold linguistics task data (PTB, CONLL-2005, and\nCONLL-2009) with 10% probability, we only take\none sentence as input that [CLS] and [SEP] are first\nand last tokens respectively.\nSince input sequence X is based on WordPiece\ntoken, we only take the last WordPiece vector of\nthe word in the last layer of Transformer encoder\nas our sole word representation for later linguistics\ntasks input to keep the same length of the token\nand label annotations.\n3.3 Transformer Encoder\nThe Transformer encoder in our model is adapted\nfrom (Vaswani et al.), which transforms the input\nrepresentation vectors into a sequence of contextu-\nalized embedding vectors with shared representa-\ntion across different tasks. We use the pre-trained\nparameters of BERT (Devlin et al., 2019) as our\nencoder initialization for faster convergence.\n3.4 Language Modeling Layers\nBERT training applies masked language modeling\n(MLM) as a training objective which corrupts the\ninput by replacing some tokens with a special token\n[MASK] and then lets the model reconstruct the\noriginal tokens. While in our LIMIT-BERT train-\ning, the linguistics specific tasks and MLM train-\n\n